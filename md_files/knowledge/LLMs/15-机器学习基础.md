---
title: 机器学习基础
sidebar: true
# isShowComments: true
---
# 机器学习基础

<ClientOnly>
<title-pv/>
</ClientOnly>

## 决策树

普通决策树（如CART决策树）和基于梯度的决策树（如梯度提升决策树，GBDT）的主要区别体现在模型构建方式和目标优化方法上。在普通决策树中，基尼系数和熵计算的公式中用到的 $ p_i $ 表示的是某个类别 $ i $ 在节点中的概率，反映了某个类别在当前节点中样本的相对频率，用于评估节点的纯度或不确定性。

### 普通决策树（CART）

普通决策树递归地对数据进行划分，基于某种标准（如基尼系数或信息增益）选择特征和分裂点构建的。其目标是最大化子节点的纯度。

**决策过程：**

- 对于每个节点，决策树尝试找到最优特征 $ j $ 和分裂点 $ s $，使得分裂后的损失函数（如基尼系数或熵）最小化。

  $$
  \min_{j, s} \left( L(\text{左子节点}) + L(\text{右子节点}) \right)
  $$

其中，损失函数 $ L $ 可能是基尼系数 $ G $ 或熵 $ H $：

- 基尼系数：$ G(p) = 1 - \sum_{i=1}^{k} p_i^2 $
- 熵：$ H(p) = -\sum_{i=1}^{k} p_i \log(p_i) $

树的构建是贪心的，每次选取当前最优分裂点。

### 回归树

回归树会尝试通过“年龄”变量来分割数据集，确保每个分割后的子集的平均收入差异最小（即最小化均方误差）

### 随机森林

随机森林是一种集成学习算法，基于决策树或回归树构建。它通过构建多个决策树或回归树，并通过投票或平均值来提高模型的稳定性和准确性。随机森林的核心思想是通过引入随机性来降低单棵树的过拟合现象，并增强模型的泛化能力。

## 梯度决策树（GBDT）

GBDT 是一种集成模型，通过不断迭代训练多个决策树来优化模型性能。与普通决策树不同，GBDT的每棵树是在前一棵树的预测残差（误差）的基础上构建的，目的是通过迭代地最小化残差来提高预测精度。

需要注意的是，GBDT并不是真正的用决策树预测结果，而是用决策树作为梯度指导去优化一个模型，使模型能达到更好的性能。

GBDT的目标是最小化整体损失函数：

$$
L = \sum_{i=1}^{n} \ell(y_i, F_M(x_i))
$$

其中：
- $ y_i $ 是实际标签，$ F_M(x_i) $ 是第 $ M $ 轮模型的预测值。
- $ \ell(y_i, F(x_i)) $ 是损失函数，常见的是均方误差（MSE）：$ \ell(y_i, F(x_i)) = (y_i - F(x_i))^2 $。
- $ F_M(x_i) = F_{M-1}(x_i) + \eta \cdot h_M(x_i) $，其中 $ h_M(x_i) $ 是第 $ M $ 棵树，$ \eta $ 是学习率。

**梯度提升步骤：**

1. **初始化模型**：首先初始化一个常数模型，通常为目标变量的均值。
   $$
   F_0(x) = \arg\min_{c} \sum_{i=1}^{n} \ell(y_i, c)
   $$
2. **计算残差**：在第 $ M $ 步，计算前一轮模型的残差（即当前的负梯度）：
   $$
   r_i^{(M)} = - \left[\frac{\partial \ell(y_i, F(x_i))}{\partial F(x_i)}\right]_{F(x_i) = F_{M-1}(x_i)}
   $$
3. **训练新的树**：使用残差 $ r_i^{(M)} $ 来拟合一棵新的决策树 $ h_M(x) $：
   $$
   h_M(x) = \arg\min_h \sum_{i=1}^{n} \left( r_i^{(M)} - h(x_i) \right)^2
   $$

4. **更新模型**：将新的树加到模型中：
   $$
   F_M(x) = F_{M-1}(x) + \eta h_M(x)
   $$
   其中 $ \eta $ 是学习率，控制步长。

#### 区别总结

- **训练方式**：普通决策树是一棵树的独立构建，而GBDT是多棵树的累加，每棵树学习的是前一棵树的残差（梯度）。
- **目标函数**：普通决策树通过熵、基尼系数等指标直接分割数据，而GBDT通过最小化整体损失函数的梯度来优化模型。
- **模型复杂度**：GBDT是多个弱学习器（决策树）的组合，而普通决策树是单棵树。GBDT通过迭代优化损失，生成一个强大的集成模型。

### Bagging (Bootstrap Aggregating)

公式：
$$
f(x) = \frac{1}{n} \sum_{i=1}^{n} T_i(x)
$$

- **核心思想**: 对数据集进行多次重采样，分别训练多个决策树模型 $T_i$，最后通过平均或投票的方式进行预测。
- **适用场景**: 当基础模型的方差较大时（易过拟合），Bagging 能通过并行训练和投票减少方差。适合随机森林（Random Forest）等。
- **优点**: 减少模型方差，适合数据量较大且无明显噪声的数据。

### LightGBM (Light Gradient Boosting Machine)

公式：
$$
f(x) = \sum_{i=1}^{N} \alpha_i T_i(x)
$$

- **核心思想**: 基于梯度提升决策树（Gradient Boosting Decision Tree, GBDT），采用 **叶子节点增长策略**（Leaf-wise Growth）和 **基于直方图的快速训练** 方法，通过梯度信息逐步优化决策树。
- **适用场景**: 当数据量较大且特征维度较高时（尤其是稀疏数据），LightGBM 在速度和内存使用上有优势。
- **优点**: 训练速度快，适合大规模、高维稀疏数据，且处理类别型特征效果较好。

### XGBoost (Extreme Gradient Boosting)

公式：
$$
f(x) = \sum_{i=1}^{N} \alpha_i T_i(x) + \lambda \sum_{i=1}^{N} \Omega(T_i)
$$

其中，$\Omega(T_i)$ 是树的复杂度惩罚项，$\lambda$ 是正则化系数。

- **核心思想**: 基于梯度提升框架的加速版本，强调 **正则化** 和 **损失函数的优化**，防止过拟合。使用逐步构建树的方式，优化残差。
- **适用场景**: 当模型容易过拟合时，XGBoost 提供强大的正则化能力。适合复杂特征和需要精度高的场景（如比赛、精准预测）。
- **优点**: 性能优化较好，适合处理缺失数据和噪声较大的数据，具有强大的正则化机制防止过拟合。

---
#### 总结
- **Bagging**：并行训练多个决策树，适合大数据且易过拟合的场景。
- **LightGBM**：基于梯度提升的决策树，适合高维稀疏数据，速度快。
- **XGBoost**：正则化加强的梯度提升树，适合需要高精度和防过拟合的场景。

逻辑回归
SVM
决策树、随机森林
梯度决策树
GBDT和


<ClientOnly>
  <leave/>
</ClientOnly/>


