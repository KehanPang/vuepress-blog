---
title: 大模型综合业务场景
sidebar: true
# isShowComments: true
---
# 大模型综合业务场景

<ClientOnly>
<title-pv/>
</ClientOnly>

## LoRA
* LoRA的超参数有哪些？可训练的参数有哪些？
* LongLoRA是什么？
* LoRA的秩通常选为多少？
* LoRA通常有两种实现方式，分别是怎么实现的？
* QLoRA是什么？
* QLoRA的nf4和fp32不能直接转换？为什么中间还有bf16？
* 讲一下混合精度训练的原理？


## 大模型
* 扩充LLM上下文的方法
* 如何让大模型接受更长的文本？
* 长文本任务上，什么时候需要微调？什么时候不需要？
* 大模型如何判断该停止生成了？
* 灾难性遗忘的解决办法（领域知识训练后，通用知识遗忘解决办法）
* 大模型在SFT时是在学习什么
* 大模型的训练目标函数
* 大模型复读机问题的解决办法
* 大模型幻觉通常有哪些表现？
    * 答非所问
    * 遗忘上下文
    * 偏离事实
* 大模型幻觉的产生原因
    * 数据质量差
    * 大模型高估了自己的能力，不知道问题边界
    * 关键信息忽视
    * 由于自回归的特性，如果一开始就错了，大模型之后只会将错就错
* 如何估算大模型训练或推理时，加载后要消耗多少显存？
* 大模型训练时瓶颈是在哪里？哪部分最耗时？
* 这些显存都用来存什么？
* 为什么大模型不擅长逻辑推理和逻辑计算
* 大模型训练时的最大困难
    * 训练困难
    * 语料污染
    * 幻觉、灾难性遗忘
* 讲讲你对SFT和RLHF的理解
* 讲讲DPO、PPO的区别
* Reward模型了解吗？
* 除了DPO还有哪些对齐算法？
* LlaMA的分组查询注意力和Mistral有什么不同？
* 温度系数为什么可以影响输出?(还有topk、topp)
* Deepspeed、Peft、vLLM是怎么实现数据并行、模型并行的？
* MOE最开始时，各个专家结构和参数规模一样吗？
* Self-Instruct的原理？
* LLM的prompt构建技巧？
* mmoe是什么？
* PEFT微调和优缺点？
* 介绍一下InstructionGPT
* 普通模型的Query Fine-tune和SFT有什么区别？
* GPT4o做了哪些优化加快了推理速度？

## BERT相关

* BERT、GPT训练时mask该怎么用？
* GPT4对比GPT3.5的提升主要来自于哪些方面？
* GPT3.5对比GPT3的提升主要来自于哪些方面？
* 为什么用BGE？有什么优势？
* BERT和RoBERTa的区别？
* BERT的几种Mask的作用是什么？
* BERT预训练时的损失函数？
* BERT和ELMo的区别？
* 文本特征提取器是什么？
* BERT训练时的Worm-up

## RAG

* LLM知识如果和RAG冲突该怎么办
* 长文本LLM和RAG该怎么选择
* RAG的准确率、召回率怎么计算？
* 困惑度怎么计算？
* 多路召回和重排序？
* Faiss向量库的原理？
* BLEU的缺点？
* COS距离和欧氏距离是否同步增减？

## 深度学习
* PostNorm和PreNorm哪个更好？了解DeepNorm吗？
* 介绍一下 CLIP 模型？说说你认为 CLIP 为什么会这么强大？
* 聊一下你知道的推荐系统的深度学习模型？
* LSTM、GRU的区别？
* Word2vec是怎么来的？
* 讲讲Embedding的原理？这些模型通常是怎么训练的？
* ddim和ddpm的原理？
* SAM的原理？
* 激活函数的作用？
* LayerNorm有哪些形式？
* SENet和CBAM提出了什么算法？

## 机器学习
* DBSCAN和KMeans区别？
* Gradient checkpoints 节省内存的原理是啥？
* XGBoost只能用于数值的残差估计吗，可否用于特征？
* XGBoost怎么填充缺失值？
* 随机森林如何保证每个树的随机性（数据、特征筛选，先随机筛选特征在取最优的）？

## Transformer

* Softmax可以并行计算吗？
* Softmax的指数上溢该怎么解决？
* 了解加法注意力吗？
* 讲讲Flash-Attention？
* 讲讲Paged-Attention？
* Kv-cache是什么？
* Sparse Attention是什么？
* Transformer有哪些加速、防止过拟合的手段？
* RoPE的虚部、实部有什么含义？
* RoPE是加性编码还是乘性编码？
* MLA怎么和RoPE结合？
* 绝对、相对编码的优势和不足？
* Transformer和Llama的LN、FFN有什么区别
* 和Seq2Seq的区别
    * 解码方法
    * Viterbi算法
* 多头注意力机制的好处？
* 分组注意力机制的好处？
* Q，K，V的理解
* 除以平方根的目的
* Sinusoidal、RoPE、ALiBi
* 是否了解长度外推与插值
* Transformer的权重共享
* Transformer的并行性
* Transformer和RWKV、RNN、Informer
* Transformers库的generate接口实现的repetition_penalty存在的问题
* 为什么初代GPT的性能比BERT差
    * GPT预训练时的任务更难（BERT的base就是为了和GPT对比，参数设定几乎一样）
    * BERT预训练用的数据集大小几乎是GPT的四倍

## 综合问题
* Scaling-Law是什么？
* Loss Spike问题的解决办法？
* 如何把英文大模型转为中文大模型（领域词表扩建、预训练、SFT）
* 想让模型适应新领域，应该预训练还是微调还是RAG
* 涌现的原因
* 前馈层在不同位置的作用？
* 对多模态模型的理解
* 多轮对话如何微调？
* 如何解决多轮对话遗忘前面对话的问题？
* CLS等标记的作用
* Llama和GPT预训练时有什么区别？
* 样本不均衡时该怎么办？
* Focal loss是什么？

## 手撕代码
* 假设给你两个链表，如何找到公共节点，只能是Y字型？
* 元组的元素必须是不可变吗？
* 讲讲堆排序、优先队列的思想
* 什么是稳定排序？
* 平衡二叉树的原理？
* 如何找到一个数组的中位数？（复杂度N，分治法）
* 打家劫舍
* 矩阵置零
* 跳跃游戏
* Shuffle数组
* 手撕BERT+LoRA
* ndcg、auc、roc怎么算？
* Louvain算法的核心思想？还知道哪些社区算法？

## 自由问题
* 对o1、o1-mini是怎么看的？
* 你认为明年大模型发展的趋势？
* 你怎么看待我们的团队、公司？有哪些好的、坏的点？
* 为什么去年openai的chatgpt成功了，但是gpt2和instructgpt没有？


<ClientOnly>
  <leave/>
</ClientOnly/>


