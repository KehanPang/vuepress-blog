---
title: 大模型综合业务场景
sidebar: true
# isShowComments: true
---
# 大模型综合业务场景

<ClientOnly>
<title-pv/>
</ClientOnly>

LoRA超参数有什么，可训练的参数有什么
Scaling-Law
Loss Spike问题的解决办法
如何让大模型接受更长的文本
如何判断大模型该停止生成了
LLM知识如果和RAG冲突该怎么办
长文本LLM、RAG
扩充LLM上下文的方法
如何把英文大模型转为中文大模型（词表适配、预训练、SFT）
灾难性遗忘的解决办法
领域知识训练后，通用知识遗忘解决办法
    领域词表扩建
想让模型适应新领域，应该预训练还是微调还是RAG
大模型在SFT时是在学习什么
大模型的训练目标函数
大模型复读机问题的解决办法
幻觉的产生原因
    数据质量
    大模型高估了自己的能力，不知道问题边界
    对齐问题
    将错就错

大模型幻觉的解决办法
    答非所问
    遗忘上下文
    偏离事实

涌现的原因
不同大小的模型的训练时、推理时显存占用
前馈层在不同位置的作用
对多模态模型的理解
为什么大模型不擅长逻辑推理和逻辑计算
多轮对话如何微调？
如何解决多轮对话遗忘前面对话的问题？
训LLM最大困难
CLS等标记的作用
Llama和GPT预训练时有什么区别？
样本不均衡时该怎么办？
Focal loss是什么？
BERT、GPT训练时mask该怎么用？
GPT4对比GPT3.5的提升主要来自于哪些方面？
GPT3.5对比GPT3的提升主要来自于哪些方面？
讲讲你对SFT和RLHF的理解
困惑度怎么计算？
RAG的准确率、召回率怎么计算？
PostNorm和PreNorm哪个更好？了解DeepNorm吗？
Softmax可以并行计算吗？
Softmax的指数上溢该怎么解决？
了解加法注意力吗？
SENet和CBAM提出了什么算法？
DBSCAN和KMeans区别？
gradient checkpoints 节省内存的原理是啥？
讲一下混合精度训练的原理？
介绍一下 CLIP 模型？说说你认为 CLIP 为什么会这么强大？
为什么用BGE？
XGBoost只能用于数值的残差估计吗，可否用于特征？
XGBoost怎么填充缺失值？
随机森林如何保证每个树的随机性（数据、特征筛选，先随机筛选特征在取最优的）？
聊一下你知道的推荐系统的深度学习模型？
假设给你两个链表，如何找到公共节点，只能是Y字型的？
如何找到一个数组的中位数？（复杂度N，分治法）
多路召回和重排序？
大模型存在的问题？
- 训练困难
- 语料污染
- 幻觉、灾难性遗忘
讲讲DPO、PPO的区别
Reward模型了解吗？
Word2vec是怎么来的？
讲讲Embedding的原理？
如何扩展长文本？
你认为明年大模型发展的趋势？
你怎么看待我们的团队、公司？好的、坏的点？
大模型训练时瓶颈是在哪里？哪部分最好耗时？
长文本任务上，什么时候需要微调？什么时候不需要？
ddim和ddpm的原理？
SAM的原理？
LLM的prompt构建技巧？
Faiss向量库的原理？
讲讲LongLoRA
Sparse Attention是什么？
Transformer有哪些加速、防止过拟合的手段？


<ClientOnly>
  <leave/>
</ClientOnly/>


