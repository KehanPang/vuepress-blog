---
title: 大模型训练
sidebar: true
# isShowComments: true
---
# 大模型训练

<ClientOnly>
<title-pv/>
</ClientOnly>


## 数据准备

数据通常分为通用数据，如网页、书籍、对话；以及专业数据，如多语言数据、科学文本（学术论文）、代码等等

## 数据清洗

为保证大模型使用高质量语料训练，通常需要质量过滤，包括冗余去除、隐私消除、词元切分等步骤

## 数据选取、划分

### Scaling Law
对于计算量$C$，模型参数量$N$，和数据集大小$D$，当不受其他两个因素制约时，模型性能$L$与每个因素都呈现幂律关系，且随着每个因素的量增大，对模型的效果提升越有限。目前已经有研究证明，模型参数量上升时，数据量也需要等比例上升。

### 数据选取方法
* Data Mixing Laws：通过在小规模数据和模型上进行实验，利用训练步数、模型大小和数据混合比例的缩放定律（Scaling Laws），来预测在大规模数据和大型模型上的性能
* DoGE/LESS：重点学习对整体梯度贡献较大的领域，可以使用影响函数来量化每条训练数据对模型的影响。但通常在大模型上直接计算影响函数是困难的，因此可以先知识蒸馏，在小模型上用影响函数分析什么样的数据是关键的
* REGMIX：使用多种数据配比训练一组小型模型，并拟合一个回归模型来预测给定各自配比的模型的性能

## 训练过程

### Pre-train

预训练阶段是通过大规模无监督学习来训练模型，让模型能够从海量的文本数据中学习语言的基本模式、语义和结构。通过学习大量文本，模型掌握语言的句法、词法，甚至潜在的语义关联。通常，预训练需要大量的数据和计算资源来处理巨大的参数量。模型在没有标注数据的情况下学习，需要高质量的数据清洗和设计合适的任务来让模型能够充分理解语言。

### SFT (Supervised Fine-Tuning)

在这个阶段，预训练好的模型会通过特定的任务和标注数据进行微调。这个步骤的目的是让模型在预训练的基础上，针对具体应用场景（如分类、问答、翻译等）进行适应性优化。

相比完全从头训练一个新模型，微调在时间和计算资源上更加经济高效。但有可能出现过拟合以及灾难性遗忘，即微调后的模型可能在特定领域表现良好，但在其他领域的表现可能会下降。

### RLHF

<div style="text-align: center;">
<img src="/img/dpoppo.png" style="margin-bottom: -20px;" width="100%" height="100%">
</div>

在LLM对齐问题上，OpenAI提出的RLHF训练范式最为人熟知，同时也是ChatGPT行之有效的对齐方案。

RLHF通常包含三个步骤：SFT, Reward Model, PPO, 该方案优点不需多说，缺点也很明显：训练流程繁琐、算法复杂、超参数多和计算量大，因此RLHF替代方案层出不穷。

#### PPO (Proximal Policy Optimization)
PPO 是一种在线强化学习方法，的核心是训练一个能更好刻画人类偏好的 Reward Model，然后使用这个 Reward Model 来显性评估模型生成结果的好坏，最终指导模型的微调。

PPO 主要缺点是需要训练单独的 Reward Model，其成本高昂并且需要大量额外数据。而 Reward Model 的好坏也直接影响了最终模型的效果。模型也有可能仅仅去学习如何讨好强化学习模型，而没有真正理解答案间的语义差距。

#### DPO (Direct Preference Optimization)

DPO 是一种离线强化学习方法，无需显性构建 Reward Model，而是直接根据偏好数据来优化策略。它巧妙地绕过了构建奖励模型和强化学习这两个的繁琐过程，直接通过偏好数据进行微调，效果简单粗暴，在使模型输出更符合人类偏好的同时，极大地缩短了训练时间和难度。

然而，四川大学的 Duanyu Feng 最近的研究《Towards Analyzing and Understanding the Limitations of DPO: A Theoretical Perspective》表明：DPO 损失函数降低产生人类不喜欢的 response 的概率的速度比增加产生人类偏好的 response 的概率的速度更快，这导致：

* DPO 阻碍了 LLM 产生人类偏好的 response 的学习能力
* DPO 对 SFT 的有效性很敏感

#### 两者的优势与不足

PPO 和 DPO 作为 LLMs Alignment 的主流算法有其各自的优势。PPO 训练一个单独的 Reward Model 来预测人类偏好。然后使用这个 Reward Model 来微调 LLMs。DPO 使用 KL constraint 直接导出最优策略，无需单独的 Reward Model。简化了 LLMs 训练流程，计算效率更高。DPO 在计算、速度和工程工作方面更加高效，但在模型效果上 PPO 要优于 DPO。在实际应用中可以根据具体需求进行选择。

## 微调技术

### Adapter tuning
Adapter tuning通过在预训练模型的每一层中插入一个小的适配器模块来实现微调。这些适配器模块包含少量可训练参数，它们在微调过程中被更新，而原始模型的参数保持不变。适配器通常由两个线性层组成，中间可能有一个非线性激活函数。

假设原始模型的第$l$层的输出为$H_l$，则经过适配器后的输出$H'_l$可以表示为：$H'l = F(H_lW_{down} + b_{down})W_{up} + b_{up}$
其中，$F$是非线性激活函数，$W_{down}和b_{down}$是降维层的权重和偏置，$W_{up}$和$b_{up}$是升维层的权重和偏置。

假设我们有一个预训练的BERT模型，我们想要对其进行适配器微调以适应一个新的文本分类任务。我们会在BERT的每一层后添加适配器模块，并只训练这些模块的参数，而不改变BERT的原始参数。

$$$ Prompt tuning
基本原理:
Prompt tuning是一种利用自然语言提示(prompt)来引导预训练语言模型的方法。这种方法不需要修改模型的内部参数，而是通过设计合适的提示来激发模型对特定任务的理解和执行。
公式表示:
Prompt tuning没有特定的数学公式，因为它更多地依赖于文本提示的设计。但是，可以将提示视为模型输入的一部分，即：
\[ Input = [Prompt; OriginalInput] \]
其中，\(Prompt\)是设计好的提示文本，\(OriginalInput\)是原始的输入文本。
具体例子:
假设我们要使用GPT-3进行问答任务，我们可以设计一个提示：“请回答以下问题：”，然后将这个问题作为输入的一部分，连同原始问题一起输入到GPT-3中。
Prefix tuning
基本原理:
Prefix tuning是Prompt tuning的一种变体，它在输入序列的前面添加一组可学习的虚拟token作为前缀。这些虚拟token在训练过程中被优化，以更好地引导模型的输出。
公式表示:
假设原始输入序列的长度为\(n\)，我们添加了长度为\(m\)的可学习前缀\(P\)，则模型的输入可以表示为：
\[ Input = [P; OriginalInput] \]
其中，\(P\)是可学习的前缀，\(OriginalInput\)是原始的输入序列。
具体例子:
假设我们要对一个生成模型进行微调，以便它能够生成特定风格的文本。我们可以添加一个可学习的前缀，这个前缀在训练过程中被优化，以使模型生成符合特定风格的文本。
Fine tuning
基本原理:
Fine tuning是指在预训练模型的基础上，对模型的所有或部分参数进行进一步的训练，以适应特定的任务。
公式表示:
Fine tuning没有特定的数学公式，因为它涉及到整个模型参数的更新。但是，可以将其视为模型参数的更新过程：
\[ W{new} = W{pretrained} + \Delta W \]
其中，\(W{new}\)是微调后的模型参数，\(W{pretrained}\)是预训练模型的参数，\(\Delta W\)是参数更新量。
具体例子:
假设我们有一个预训练的ResNet模型，我们想要将其用于一个新的图像识别任务。我们会使用新的数据集对ResNet的所有层进行微调，以使其更好地适应新任务。
P-tuning
基本原理:
P-tuning是一种将Prompt转换为可学习的Embedding的方法。它使用一个小型的神经网络来生成Prompt的Embedding，然后将这些Embedding与输入数据一起送入预训练模型。
公式表示:
假设Prompt的Embedding由一个小型神经网络\(G\)生成，那么模型的输入可以表示为：
\[ Input = [G(Prompt); OriginalInput] \]
其中，\(G\)是生成Prompt Embedding的神经网络，\(Prompt\)是设计好的提示文本，\(OriginalInput\)是原始的输入文本。
具体例子:
假设我们要使用BERT进行情感分析任务，我们可以设计一个提示，并通过一个小的神经网络将其转换为Embedding，然后将这个Embedding与文本输入一起送入BERT。
Freeze tuning
基本原理:
Freeze tuning是指在微调过程中冻结模型的大部分参数，只训练模型的一小部分参数，通常是最后一层或者特定的几层。
公式表示:
Freeze tuning没有特定的数学公式，但它涉及到模型参数的选择性更新：
\[ W{new} = \{W{pretrained} + \Delta W | W \in trainable\layers\} \]
其中，\(trainable\layers\)是选择进行微调的层。
具体例子:
假设我们有一个预训练的Transformer模型，我们想要将其用于一个新的机器翻译任务。我们可能会冻结模型的大部分层，只对最后一层进行微调。
LoRA
基本原理:
LoRA（Low-Rank Adaptation）是一种通过低秩矩阵分解来微调模型的方法。它通过添加一对低秩矩阵来近似参数更新，从而减少需要训练的参数数量。
公式表示:
假设原始参数矩阵为\(W\)，LoRA通过添加低秩矩阵\(B\)和\(A\)来近似参数更新：
\[ W{new} = W + BA \]
其中，\(B\)和\(A\)是低秩矩阵，它们的乘积近似于参数更新\(\Delta W\)。
具体例子:
假设我们有一个预训练的BERT模型，我们想要对其进行LoRA微调以适应一个新的文本分类任务。我们会在BERT的每一层的参数更新中应用LoRA，只训练低秩矩阵\(B\)和\(A\)，而不是整个参数矩阵\(W\)。
QLoRA
基本原理:
QLoRA（Quantized LoRA）是LoRA的一个变体，它结合了量化技术来进一步减少模型的大小和提高微调效率。QLoRA在微调过程中使用量化技术来减少模型参数的位数，从而减少内存占用和计算需求。
公式表示:
QLoRA的公式与LoRA相似，但是在参数更新时会应用量化操作：
\[ W{new} = Q(W + BA) \]
其中，\(Q\)表示量化操作，\(W\)是原始参数矩阵，\(B\)和\(A\)是低秩矩阵。
具体例子:
假设我们有一个预训练的大型语言模型，如GPT-3，我们想要对其进行QLoRA微调以适应一个新的对话生成任务。我们会在GPT-3的每一层的参数更新中应用QLoRA，只训练低秩矩阵\(B\)和\(A\)，并在更新后对参数进行量化处理。


## Tokenizer
### Byte-Pair Encoding(BPE)
### WordPiece
### SentencePiece
### Unigram
### 词汇表不全、过大问题
### 各路LLM的Tokenizer

## 训练损失函数

### BERT

* MLM (Masked Language Model)：在MLM任务中掩盖部分词汇，并通过最大化掩盖词汇的预测概率来进行训练，假设$T$是被掩盖的词语集合，$p_\theta(w_t|C)$是模型在上下文$C$下预测词汇$w_t$的概率，那么MLM损失为$L_{MLM}=-\Sigma_{t \in T}log p_\theta(w_t|C)$
* NSP (Next Sentence Prediction)：NSP任务用于预测两句话是否连续，假设$y \in \{0, 1\}$表示标签，$p_\theta(y|A,B)$是模型预测两句$A$和$B$是否连续的概率，那么NSP损失函数可以表示为：$L_{NSP}=-log p_\theta(y|A,B)$$

### GPT

GPT 使用 自回归语言模型 (Autoregressive Language Model) 的损失函数。其目标是根据前面的词预测下一个词。模型根据已知的词来最大化预测下一个词的出现概率。

<ClientOnly>
  <leave/>
</ClientOnly/>