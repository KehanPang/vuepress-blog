---
title: Transformer
sidebar: true
# isShowComments: true
---
# Transformer

<ClientOnly>
<title-pv/>
</ClientOnly>


## 手撕Transformer代码


Transformer和Llama的LN、FFN有什么区别
和Seq2Seq的区别
    解码方法
    Viterbi算法

多头注意力
Q，K，V的理解
除以平方根的目的
位置编码与旋转编码
    Sinusoidal、RoPE、ALiBi
    绝对编码与相对编码
    长度外推与插值
    两者的优劣
    ALiBi (Attention with Linear Biases)

自注意力机制
掩码注意力机制
Transformer的权重共享
Transformer的并行性
Transformer和RWKV、RNN、Informer
Transformers库的generate接口实现的repetition_penalty存在的问题
BERT和RoBerta的区别
BERT和GPT的区别
BERT的双向体现在哪里
BERT是如何训练的
BERT如何解决长文本问题
BERT训练时的Worm-up
为什么初代GPT的性能比BERT差
    GPT预训练时的任务更难（BERT的base就是为了和GPT对比，参数设定几乎一样）
    BERT预训练用的数据集大小几乎是GPT的四倍

GPT4o做了哪些优化加快了推理速度？


<ClientOnly>
  <leave/>
</ClientOnly/>


