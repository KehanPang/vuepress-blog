---
title: 深度学习基础
sidebar: true
# isShowComments: true
---
# 深度学习基础

<ClientOnly>
<title-pv/>
</ClientOnly>

## Norm方式的区别

* Layer-Norm
* Batch-Norm
* RMS-Norm
* DeepNorm

## 正则化

## 前向、反向传播
* 学习率
* 权重衰减
* 梯度消失
* 梯度饱和
* 梯度爆炸

## 经典模型
* AE、VAE
* GAN
* Diffusion
* GNNs
  * GCN
  * GAT
  * GraphSAGE
  * GIN
  * HAN
  * MAGNN
为什么GCN要除以度矩阵的负二分之一次方

## Dropout
## 残差、特征连接

## 激活函数
* Sigmoid
* Tanh
* ReLU
* LeakyReLU
* ELU
* GLU
* GELU
* Swish
* SwiGLU

## 损失函数
* MSE
* 交叉熵

## 优化器
* SGD
* Adam




<ClientOnly>
  <leave/>
</ClientOnly/>


