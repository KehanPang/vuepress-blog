---
title: 机器学习基础
sidebar: true
# isShowComments: true
---
# 机器学习基础

<ClientOnly>
<title-pv/>
</ClientOnly>

## 决策树

普通决策树（如CART决策树）和基于梯度的决策树（如梯度提升决策树，GBDT）的主要区别体现在模型构建方式和目标优化方法上。下面从公式层面来分析两者的差异。

## 普通决策树（CART）

普通决策树是通过递归地对数据进行划分，基于某种标准（如基尼系数或信息增益）选择特征和分裂点构建的。其目标是最大化子节点的纯度。

**决策过程：**

- 对于每个节点，决策树尝试找到最优特征 <smalltex> j </smalltex> 和分裂点 <smalltex> s </smalltex>，使得分裂后的损失函数（如基尼系数或熵）最小化。

  <div style="text-align: center;"><tex>
  \min_{j, s} \left( L(\text{左子节点}) + L(\text{右子节点}) \right)
  </tex></div>

其中，损失函数 <smalltex> L </smalltex> 可能是基尼系数 <smalltex> G </smalltex> 或熵 <smalltex> H </smalltex>：

- 基尼系数：<smalltex> G(p) = 1 - \sum_{i=1}^{k} p_i^2 </smalltex>
- 熵：<smalltex> H(p) = -\sum_{i=1}^{k} p_i \log(p_i) </smalltex>

树的构建是贪心的，每次选取当前最优分裂点。

### 基于梯度的决策树（GBDT）

GBDT 是一种集成模型，通过不断迭代训练多个决策树来优化模型性能。与普通决策树不同，GBDT的每棵树是在前一棵树的预测残差（误差）的基础上构建的，目的是通过迭代地最小化残差来提高预测精度。

GBDT的目标是最小化整体损失函数：

<div style="text-align: center;"><tex>
L = \sum_{i=1}^{n} \ell(y_i, F_M(x_i))
</tex></div>

其中：
- <smalltex> y_i </smalltex> 是实际标签，<smalltex> F_M(x_i) </smalltex> 是第 <smalltex> M </smalltex> 轮模型的预测值。
- <smalltex> \ell(y_i, F(x_i)) </smalltex> 是损失函数，常见的是均方误差（MSE）：<smalltex> \ell(y_i, F(x_i)) = (y_i - F(x_i))^2 </smalltex>。
- <smalltex> F_M(x_i) = F_{M-1}(x_i) + \eta \cdot h_M(x_i) </smalltex>，其中 <smalltex> h_M(x_i) </smalltex> 是第 <smalltex> M </smalltex> 棵树，<smalltex> \eta </smalltex> 是学习率。

**梯度提升步骤：**

1. **初始化模型**：首先初始化一个常数模型，通常为目标变量的均值。

   <div style="text-align: center;"><tex>
   F_0(x) = \arg\min_{c} \sum_{i=1}^{n} \ell(y_i, c)
   </tex></div>

2. **计算残差**：在第 <smalltex> M </smalltex> 步，计算前一轮模型的残差（即当前的负梯度）：

   <div style="text-align: center;"><tex>
   r_i^{(M)} = - \left[\frac{\partial \ell(y_i, F(x_i))}{\partial F(x_i)}\right]_{F(x_i) = F_{M-1}(x_i)}
   </tex></div>

3. **训练新的树**：使用残差 <smalltex> r_i^{(M)} </smalltex> 来拟合一棵新的决策树 <smalltex> h_M(x) </smalltex>：

   <div style="text-align: center;"><tex>
   h_M(x) = \arg\min_h \sum_{i=1}^{n} \left( r_i^{(M)} - h(x_i) \right)^2
   </tex></div>

4. **更新模型**：将新的树加到模型中：

   <div style="text-align: center;"><tex>
   F_M(x) = F_{M-1}(x) + \eta h_M(x)
   </tex></div>

   其中 <smalltex> \eta </smalltex> 是学习率，控制步长。

### 区别总结

- **训练方式**：普通决策树是一棵树的独立构建，而GBDT是多棵树的累加，每棵树学习的是前一棵树的残差（梯度）。
- **目标函数**：普通决策树通过熵、基尼系数等指标直接分割数据，而GBDT通过最小化整体损失函数的梯度来优化模型。
- **模型复杂度**：GBDT是多个弱学习器（决策树）的组合，而普通决策树是单棵树。GBDT通过迭代优化损失，生成一个强大的集成模型。



在普通决策树中，基尼系数和熵计算的公式中用到的 <smalltex> p_i </smalltex> 表示的是某个类别 <smalltex> i </smalltex> 在节点中的概率，反映了某个类别在当前节点中样本的相对频率，用于评估节点的纯度或不确定性。

逻辑回归
SVM
决策树、随机森林
梯度决策树
GBDT和Bagging
LightGBM、XGBoost


<ClientOnly>
  <leave/>
</ClientOnly/>


