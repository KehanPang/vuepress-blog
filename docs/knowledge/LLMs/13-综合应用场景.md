---
title: 大模型综合业务场景
sidebar: true
# isShowComments: true
---
# 大模型综合业务场景

<ClientOnly>
<title-pv/>
</ClientOnly>

LoRA超参数有什么，可训练的参数有什么
Scaling-Law
Loss Spike问题的解决办法
如何让大模型接受更长的文本
如何判断大模型该停止生成了
LLM知识如果和RAG冲突该怎么办
长文本LLM、RAG
扩充LLM上下文的方法
如何把英文大模型转为中文大模型（词表适配、预训练、SFT）
灾难性遗忘的解决办法
领域知识训练后，通用知识遗忘解决办法
    领域词表扩建
想让模型适应新领域，应该预训练还是微调还是RAG
大模型在SFT时是在学习什么
大模型的训练目标函数
大模型复读机问题的解决办法
幻觉的产生原因
    数据质量
    大模型高估了自己的能力，不知道问题边界
    对齐问题
    将错就错

大模型幻觉的解决办法
    答非所问
    遗忘上下文
    偏离事实

涌现的原因
不同大小的模型的训练时、推理时显存占用
前馈层在不同位置的作用
对多模态模型的理解
为什么大模型不擅长逻辑推理和逻辑计算
多轮对话如何微调？
如何解决多轮对话遗忘前面对话的问题？
训LLM最大困难
CLS等标记的作用
Llama和GPT预训练时有什么区别？
样本不均衡时该怎么办？
Focal loss是什么？
BERT、GPT训练时mask该怎么用？
GPT4对比GPT3.5的提升主要来自于哪些方面？
GPT3.5对比GPT3的提升主要来自于哪些方面？
讲讲你对SFT和RLHF的理解
困惑度怎么计算？
RAG的准确率、召回率怎么计算？
PostNorm和PreNorm哪个更好？了解DeepNorm吗？

<ClientOnly>
  <leave/>
</ClientOnly/>


